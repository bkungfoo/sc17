{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feline Neural Network\n",
    "\n",
    "**Author(s):** bfoo@google.com, kozyr@google.com, ronbodkin@google.com\n",
    "\n",
    "**Reviewer(s):** \n",
    "\n",
    "Let's export our model into a protobuf format that can be used to serve our tensorflow model. We are going to make inference calls for an image or set of images over the web!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your username:\n",
    "YOUR_GMAIL_ACCOUNT = '******' # Whatever is before @gmail.com in your email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for this section:\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Serving Setup\n",
    "\n",
    "To create a TF model for Tensorflow serving, we will be using the estimator.export_savedmodel() method. \n",
    "\n",
    "During training, estimator checkpoints contain saved values for variables (e.g. weights and biases), which can be restored as long as the estimator loading the checkpoint contains the same variable names. We can copy-paste the estimator, model_fn, cnn, etc from steps 8-9 below, and exporting the model will work correctly.\n",
    "\n",
    "However, we demonstrate below that the estimator need not be identical to that in training, and probably should not be since sending prediction requests over the web requires not only flattening input tensors, but placing them within a dictionary. Consequently, our model_fn will actually extract and reshape the tensors prior to running them through the cnn. As long as our neural net is identical to the neural net used to train the model, we can still restore the checkpoints correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join('/home', YOUR_GMAIL_ACCOUNT, 'data/output_cnn_big/')  # Directory where we store our logging and models.\n",
    "SERVING_DIR = os.path.join('/home', YOUR_GMAIL_ACCOUNT, 'data/serving_cnn_model') # Where we will export our model for serving\n",
    "\n",
    "# TensorFlow Serving Setup:\n",
    "NUM_CLASSES = 2  # Identical to training!\n",
    "\n",
    "# Network parameters: must be identical to training\n",
    "CNN_KERNEL_SIZE = 3  # Receptive field will be square window with this many pixels per side.\n",
    "CNN_STRIDES = 2  # Distance between consecutive receptive fields.\n",
    "CNN_FILTERS = 16  # Number of filters (new receptive fields to train, i.e. new channels) in first convolutional layer.\n",
    "FC_HIDDEN_UNITS = 512  # Number of hidden units in the fully connected layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the pixel dimensions for the network are identical to what is used in the training, validation, and testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_dim = [128, 128]\n",
    "pixels = pixel_dim[0] * pixel_dim[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN architecture\n",
    "\n",
    "**IMPORTANT**: The cnn() function defines the neural network structure and must be IDENTICAL to that used to train the weights and biases at every layer. This is because we must load the values of saved variables (weights and biases) from the checkpoint into the same components in the network.\n",
    "\n",
    "However, we no longer need some of the parameters such as 'dropout' (not used), 'reuse' (set to AUTO_REUSE), or 'is_training' (always false), since we are merely loading the values once and running predictions with these values. Consequently, we have removed these arguments from cnn() and explicitly assigned them below to simplify the code. Look for **\"NOTE:\"** in the comments below for changes, and compare with the definition in step_8_to_9.ipynb.\n",
    "\n",
    "Nevertheless, if you kept the network definition identical to that during training, everything will still work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architecture\n",
    "# NOTE: only images is required, all other arguments removed!\n",
    "def cnn(images):\n",
    "  \"\"\"Defines the architecture of the neural network.\n",
    "  \n",
    "  Will be called within generate_model_fn() below.\n",
    "  \n",
    "  Args: \n",
    "    image: set of images as 4-d tensor (of batch_size) pulled in when_input_fn() is executed.\n",
    "    \n",
    "  Returns:\n",
    "    2-d tensor: each images [logit(1-p), logit(p)] where p=Pr(1),\n",
    "                i.e. probability that class is 1 (cat in our case).\n",
    "                Note: logit(p) = logodds(p) = log(p / (1-p))\n",
    "  \"\"\"\n",
    "\n",
    "  # NOTE: Reuse the variables if they exist. They will be created once upon loading the checkpoint, then reused afterwards.\n",
    "  with tf.variable_scope('cnn', reuse=tf.AUTO_REUSE):\n",
    "    layer_1 = tf.layers.conv2d(\n",
    "      inputs=images,\n",
    "      kernel_size=CNN_KERNEL_SIZE,\n",
    "      strides=CNN_STRIDES,\n",
    "      filters=CNN_FILTERS,\n",
    "      padding='SAME',\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    layer_2 = tf.layers.conv2d(\n",
    "      inputs=layer_1,\n",
    "      kernel_size=CNN_KERNEL_SIZE,\n",
    "      strides=CNN_STRIDES,\n",
    "      filters=CNN_FILTERS * (2 ** 1),\n",
    "      padding='SAME',\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    layer_3 = tf.layers.conv2d(\n",
    "      inputs=layer_2,\n",
    "      kernel_size=CNN_KERNEL_SIZE,\n",
    "      strides=CNN_STRIDES,\n",
    "      filters=CNN_FILTERS * (2 ** 2),\n",
    "      padding='SAME',\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    layer_4 = tf.layers.conv2d(\n",
    "      inputs=layer_3,\n",
    "      kernel_size=CNN_KERNEL_SIZE,\n",
    "      strides=CNN_STRIDES,\n",
    "      filters=CNN_FILTERS * (2 ** 3),\n",
    "      padding='SAME',\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    layer_5 = tf.layers.conv2d(\n",
    "      inputs=layer_4,\n",
    "      kernel_size=CNN_KERNEL_SIZE,\n",
    "      strides=CNN_STRIDES,\n",
    "      filters=CNN_FILTERS * (2 ** 4),\n",
    "      padding='SAME',\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    layer_5_flat = tf.reshape(\n",
    "      layer_5, \n",
    "      shape=[-1,\n",
    "             CNN_FILTERS * (2 ** 4) *\n",
    "             pixels  / (CNN_STRIDES ** 5) / (CNN_STRIDES ** 5)])\n",
    "    \n",
    "    dense_layer= tf.layers.dense(\n",
    "      inputs=layer_5_flat,\n",
    "      units=FC_HIDDEN_UNITS,\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    # NOTE: this dropout_layer here is no longer needed, but we keep it here for comparison with training.\n",
    "    dropout_layer = tf.layers.dropout( \n",
    "      inputs=dense_layer, \n",
    "      rate=0.0,  # No dropouts\n",
    "      training=False)  # Never training\n",
    "\n",
    "    return tf.layers.dense(inputs=dropout_layer, units=NUM_CLASSES)  # 2-d tensor: [logit(1-p), logit(p)] for each image in batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Function\n",
    "\n",
    "We are no longer calling the *TRAIN* and *EVAL* paths, so we only need to handle the *PREDICT* mode.\n",
    "\n",
    "For serving, the `features` argument is a dictionary containing a flattened array of pixel values for the image, rather than a 4d tensor as in training. However, as long as the features are parsed out and reshaped appropriately, and passed into the cnn, the output is consistent with the original estimator.predict() in step 8 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model function:\n",
    "def generate_model_fn():\n",
    "  \"\"\"Return a function that determines how TF estimator operates.\n",
    "\n",
    "  Only PREDICT mode will be called below. The returned function _cnn_model_fn\n",
    "  below exports the predicted class and predicted probability to subsequent\n",
    "  saved protobuf model.\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    _cnn_model_fn: a function that returns specs for use with TF estimator\n",
    "  \"\"\"\n",
    "\n",
    "  def _cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"A function that determines specs for the TF estimator based on mode of operation.\n",
    "    \n",
    "    Args: \n",
    "      features: actual data containing a dictionary with a single input field 'images'\n",
    "                (which goes into scope within estimator function) as 4-d tensor (of batch_size),\n",
    "                pulled in via tf executing _input_fn(), which is the output to generate_input_fn()\n",
    "                and is in memory\n",
    "      labels: ignored since we are not training or evaluating\n",
    "      mode: TF object indicating whether we're in train, eval, or predict mode.\n",
    "      \n",
    "    Returns:\n",
    "           estim_specs: collections of metrics and tensors that are required for training (e.g. prediction values, loss value, train_op tells model weights how to update)\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: For serving, the features fed in will be a dict with a serialized image.\n",
    "    image_flattened = tf.convert_to_tensor(features['flattened_image'], dtype=tf.int64)\n",
    "    images = tf.reshape(image_flattened, [1, pixel_dim[0], pixel_dim[1], 3])\n",
    "    images = tf.to_float(images) / 255\n",
    "    # Rescale from (0,1) to (-1,1) so that the \"center\" of the image range is 0:\n",
    "    images = (images * 2) - 1\n",
    "    \n",
    "    # Use the cnn() to compute logits:\n",
    "    logits = cnn(images)\n",
    "    # We'll be evaluating these later.\n",
    "\n",
    "    # Transform logits into predictions:\n",
    "    pred_classes = tf.argmax(logits, axis=1)  # Returns 0 or 1, whichever has larger logit.\n",
    "    pred_prob = tf.nn.softmax(logits=logits)[:, 1]  # Applies softmax function to return 2-d probability vector.\n",
    "    # Note: we're not outputting pred_prob in this tutorial, that line just shows you\n",
    "    # how to get it if you want it. Softmax[i] = exp(logit[i]) / sum(exp((logit[:]))\n",
    "\n",
    "    # NOTE: For serving (prediction mode), we add a new field export_outputs, which returns\n",
    "    # a JSON of values for predicted class (0 or 1), and predicted probability of being a cat.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      return tf.estimator.EstimatorSpec(mode,\n",
    "                                        predictions=tf.stack([tf.cast(pred_classes, dtype=tf.float32)[0],\n",
    "                                                              pred_prob[0],\n",
    "                                                             ]),\n",
    "                                        export_outputs={'predictions': tf.estimator.export.PredictOutput(outputs={\n",
    "                                            'class': pred_classes,\n",
    "                                            'prob': pred_prob,\n",
    "                                        })})\n",
    "\n",
    "    # NOTE: We will not be calling the estimator using train or eval, so this path is ignored.\n",
    "    # There is no need to define the loss function, optimizer, or training operation.\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "    )\n",
    "\n",
    "  return _cnn_model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Estimator\n",
    "\n",
    "The estimator no longer needs configs, which are primarily used for saving checkpoints and outputting training and evaluation summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Estimator:\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=generate_model_fn(),  # Call our generate_model_fn to create model function\n",
    "  model_dir=MODEL_DIR,  # Where to look for model checkpoints\n",
    "  #config not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "  feature_spec = {'flattened_image': tf.FixedLenFeature(dtype=tf.int64, shape=[pixel_dim[0] * pixel_dim[1] * 3])}\n",
    "  return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export_savedmodel(export_dir_base=SERVING_DIR,\n",
    "                            serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
